# ğŸ•·ï¸ Bot de Veille Concurrentielle E-commerce

Un bot Python professionnel pour surveiller automatiquement les prix et la disponibilitÃ© des produits concurrents sur les principales plateformes e-commerce.

## ğŸ¯ FonctionnalitÃ©s Principales

### âœ… Sites SupportÃ©s
- **Shopify** (tous les sites .myshopify.com)
- **Amazon** (Amazon.fr, Amazon.com, etc.)
- **Etsy** (marketplace crÃ©ateurs)
- **Leboncoin** (petites annonces)
- **Beacon.by** (services freelances)
- **Fiverr** (services freelances)

### ğŸ“Š DonnÃ©es CollectÃ©es
- Titre du produit
- Prix actuel
- DisponibilitÃ© (en stock/rupture)
- Note et nombre d'avis
- Description courte
- Historique des changements

### ğŸš€ Modes d'Utilisation
- **Scraping ponctuel** : Analyse immÃ©diate
- **Surveillance pÃ©riodique** : ContrÃ´le automatique (cron job intÃ©grÃ©)
- **Alertes intelligentes** : Notifications par email des changements

## ğŸ› ï¸ Installation

### PrÃ©-requis
- Python 3.8+
- pip (gestionnaire de paquets Python)

### Installation des dÃ©pendances
```bash
python -m pip install -r requirements.txt
```

### Configuration (optionnel)
Copiez le fichier de configuration :
```bash
cp .env.example .env
```

Ã‰ditez `.env` pour configurer les alertes email :
```env
EMAIL_ENABLED=true
SMTP_SERVER=smtp.gmail.com
SMTP_PORT=587
EMAIL_USER=votre_email@gmail.com
EMAIL_PASSWORD=votre_mot_de_passe_app
EMAIL_RECIPIENTS=destinataire@email.com
```

## ğŸ“‹ Utilisation

### 1. PrÃ©parer le fichier d'URLs

CrÃ©ez un fichier CSV avec vos URLs Ã  surveiller :

```csv
url,name,category
https://www.amazon.fr/dp/B08N5WRWNW,Echo Dot,Ã‰lectronique
https://www.etsy.com/listing/123456789/custom-mug,Mug PersonnalisÃ©,Maison
https://example-shop.myshopify.com/products/t-shirt,T-Shirt Mode,VÃªtements
```

Ou un simple fichier TXT :
```txt
https://www.amazon.fr/dp/B08N5WRWNW
https://www.etsy.com/listing/123456789/custom-mug
https://example-shop.myshopify.com/products/t-shirt
```

### 2. Lancer le scraping

#### Scraping unique
```bash
python main.py -f data/input_urls.csv
```

#### Scraping avec rÃ©sumÃ© email
```bash
python main.py -f data/input_urls.csv --summary
```

#### Surveillance continue (toutes les 6 heures)
```bash
python main.py -f data/input_urls.csv --schedule 6
```

#### Mode verbose (dÃ©taillÃ©)
```bash
python main.py -f data/input_urls.csv --verbose
```

### 3. Consulter les sites supportÃ©s
```bash
python main.py --sites
```

## ğŸ“ Structure du Projet

```
competitive_scraper/
â”œâ”€â”€ main.py                    # Point d'entrÃ©e principal
â”œâ”€â”€ config.py                  # Configuration globale
â”œâ”€â”€ requirements.txt           # DÃ©pendances Python
â”œâ”€â”€ README.md                  # Documentation
â”œâ”€â”€ .env.example              # Template de configuration
â”œâ”€â”€ 
â”œâ”€â”€ utils/                     # Utilitaires
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ file_loader.py        # Chargement des URLs
â”œâ”€â”€ 
â”œâ”€â”€ scrapers/                  # Modules de scraping
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ base_scraper.py       # Classe de base
â”‚   â”œâ”€â”€ scraper_factory.py    # Factory pattern
â”‚   â”œâ”€â”€ shopify_scraper.py    # Scraper Shopify
â”‚   â”œâ”€â”€ amazon_scraper.py     # Scraper Amazon
â”‚   â”œâ”€â”€ etsy_scraper.py       # Scraper Etsy
â”‚   â”œâ”€â”€ leboncoin_scraper.py  # Scraper Leboncoin
â”‚   â”œâ”€â”€ beacon_scraper.py     # Scraper Beacon
â”‚   â””â”€â”€ fiverr_scraper.py     # Scraper Fiverr
â”œâ”€â”€ 
â”œâ”€â”€ reports/                   # GÃ©nÃ©ration de rapports
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ report_generator.py   # Rapports CSV et analyses
â”œâ”€â”€ 
â”œâ”€â”€ alerts/                    # SystÃ¨me d'alertes
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ notifier.py          # Notifications email
â”œâ”€â”€ 
â””â”€â”€ data/                     # DonnÃ©es
    â””â”€â”€ input_urls.csv        # Exemple d'URLs
```

## ğŸ“Š Rapports GÃ©nÃ©rÃ©s

### Rapport Principal (CSV)
- Toutes les donnÃ©es collectÃ©es
- MÃ©tadonnÃ©es de scraping (durÃ©e, succÃ¨s/Ã©chec)
- Horodatage prÃ©cis

### Rapport de Comparaison
- Changements de prix (avec pourcentages)
- Modifications de disponibilitÃ©
- Nouveaux/anciens produits

### RÃ©sumÃ© ExÃ©cutif
- Statistiques globales
- Prix moyens par site
- Taux de succÃ¨s du scraping

## ğŸ”” SystÃ¨me d'Alertes

### Alertes Prix
- Seuil de changement configurable (dÃ©faut: 5%)
- DÃ©tection hausse/baisse
- Historique des variations

### Alertes Stock
- Passages en rupture de stock
- Retour en disponibilitÃ©
- Changements de statut

### Notifications
- Email HTML formatÃ©
- RÃ©sumÃ©s pÃ©riodiques
- Rapports d'erreurs

## âš™ï¸ Configuration AvancÃ©e

### ParamÃ¨tres de Scraping
```python
# Dans config.py
DEFAULT_TIMEOUT = 10          # Timeout requÃªtes (secondes)
MAX_RETRIES = 3              # Nombre de tentatives
DELAY_BETWEEN_REQUESTS = 2   # DÃ©lai entre requÃªtes
PRICE_CHANGE_THRESHOLD = 5.0 # Seuil d'alerte prix (%)
```

### Anti-dÃ©tection
- Rotation automatique des User-Agents
- DÃ©lais alÃ©atoires entre requÃªtes
- Headers HTTP rÃ©alistes
- Gestion des codes d'erreur 403/429

## ğŸ›ï¸ Personnalisation

### Ajouter un nouveau site
1. CrÃ©er un nouveau scraper dans `scrapers/`
2. HÃ©riter de `BaseScraper`
3. ImplÃ©menter `scrape_product()`
4. Ajouter au `ScraperFactory`

```python
class MonSiteScraper(BaseScraper):
    def scrape_product(self, url, product_info=None):
        response = self._make_request(url)
        soup = self._parse_html(response)
        
        return {
            'title': self._extract_text(soup, ['h1', '.title']),
            'price': self._clean_price(price_text),
            'availability': 'En stock',
            'rating': None,
            'review_count': None,
            'description': ''
        }
```

### Personnaliser les sÃ©lecteurs
Modifiez `SITE_CONFIGS` dans `config.py` :

```python
SITE_CONFIGS = {
    'monsite': {
        'selectors': {
            'title': ['h1.product-name', '.title'],
            'price': ['.price', '.cost'],
            'availability': ['.stock'],
            'reviews': ['.rating'],
            'description': ['.desc']
        }
    }
}
```

## ğŸš€ DÃ©ploiement en Production

### Serveur Linux
```bash
# Crontab pour exÃ©cution automatique toutes les 6h
0 */6 * * * /usr/bin/python /path/to/main.py -f /path/to/urls.csv --summary
```

### Docker (optionnel)
```dockerfile
FROM python:3.9-slim
WORKDIR /app
COPY requirements.txt .
RUN pip install -r requirements.txt
COPY . .
CMD ["python", "main.py", "-f", "data/input_urls.csv", "--schedule", "6"]
```

### Variables d'environnement
```bash
export EMAIL_ENABLED=true
export SMTP_SERVER=smtp.gmail.com
export EMAIL_USER=bot@monentreprise.com
export EMAIL_PASSWORD=mot_de_passe_application
```

## ğŸ“ˆ Cas d'Usage

### Pour Dropshippers
- Surveiller les prix fournisseurs
- DÃ©tecter les ruptures de stock
- Ajuster automatiquement les marges

### Pour E-commerÃ§ants
- Analyser la concurrence directe
- Optimiser le pricing
- Identifier les opportunitÃ©s de marchÃ©

### Pour Agences Marketing
- Rapports clients automatisÃ©s
- Veille sectorielle
- Analyses comparatives

## ğŸ› DÃ©pannage

### Erreurs communes
```bash
# Erreur 403/429 (blocage)
# Solution: Augmenter les dÃ©lais dans config.py
DELAY_BETWEEN_REQUESTS = 5

# Timeout sur requÃªtes
# Solution: Augmenter le timeout
DEFAULT_TIMEOUT = 20

# SÃ©lecteurs obsolÃ¨tes
# Solution: Mettre Ã  jour SITE_CONFIGS
```

### Logs dÃ©taillÃ©s
```bash
python main.py -f data/input_urls.csv --verbose
```

Les logs sont sauvegardÃ©s dans `logs/scraper_YYYYMMDD.log`

## ğŸ“„ Licence

Ce projet est sous licence MIT. Voir `LICENSE` pour plus de dÃ©tails.

## ğŸ¤ Contribution

Les contributions sont les bienvenues ! 

1. Fork le projet
2. CrÃ©er une branche feature
3. Commit vos changements
4. Push vers la branche
5. Ouvrir une Pull Request

## ğŸ“ Support

- **Issues GitHub** : Pour les bugs et demandes de fonctionnalitÃ©s
- **Email** : support@bot-veille.com
- **Documentation** : https://docs.bot-veille.com

---

**âš¡ DÃ©veloppÃ© avec passion pour les entrepreneurs du e-commerce âš¡**